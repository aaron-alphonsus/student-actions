@inproceedings{Kehrer2013DoesIF,
    title={Does Immediate Feedback While Doing Homework Improve Learning?},
    author={P. Kehrer and K. Kelly and N. Heffernan},
    booktitle={FLAIRS Conference},
    year={2013}
}

@article{aes_perelman,
    author = {Perelman, Les},
    year = {2012},
    month = {01},
    pages = {121-131},
    title = {Construct validity, length, score, and time in holistically graded writing assessments: The case against automated essay scoring (AES)},
    journal = {International advances in writing research: Cultures, places, measures}
}

@inproceedings{sktime,
    author = {L{\"{o}}ning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor and Lines, Jason and Kir{\'{a}}ly, Franz J},
    booktitle = {Workshop on Systems for ML at NeurIPS 2019},
    title = {{sktime: A Unified Interface for Machine Learning with Time Series}},
    date = {2019},
}

@misc{quick_comments_nth, 
    url={https://www.neilheffernan.net/projects/funded-projects/quickcomments},
    author={Heffernan, Neil}
} 

 @misc{live_chart_nth, 
     title={LIVE CHART},
     url={https://www.neilheffernan.net/projects/funded-projects/live-chart}
 } 
 
@inproceedings{hmm_classification,
    author = {Blasiak, Sam and Rangwala, Huzefa},
    year = {2011},
    month = {01},
    pages = {1192-1197},
    title = {A Hidden Markov Model Variant for Sequence Classification.},
    journal = {IJCAI International Joint Conference on Artificial Intelligence},
    doi = {10.5591/978-1-57735-516-8/IJCAI11-203}
}

@article{aes,
    author = {Semire, DIKLI},
    year = {2006},
    month = {01},
    pages = {},
    title = {Automated Essay Scoring},
    volume = {7},
    journal = {The Turkish Online Journal of Distance Education}
}

@misc{smith_2018, 
    title={More States Opting To 'Robo-Grade' Student Essays By Computer}, 
    url={https://www.npr.org/2018/06/30/624373367/more-states-opting-to-robo-grade-student-essays-by-computer}, 
    journal={NPR}, 
    publisher={NPR}, 
    author={Smith, Tovia}, 
    year={2018}
} 

@article{Karim_2019,
   title={Insights Into LSTM Fully Convolutional Networks for Time Series Classification},
   volume={7},
   ISSN={2169-3536},
   url={http://dx.doi.org/10.1109/ACCESS.2019.2916828},
   DOI={10.1109/access.2019.2916828},
   journal={IEEE Access},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang},
   year={2019},
   pages={67718–67725}
}

@inproceedings{graded_openresponses,
author = {Erickson, John A. and Botelho, Anthony F. and McAteer, Steven and Varatharaj, Ashvini and Heffernan, Neil T.},
title = {The Automated Grading of Student Open Responses in Mathematics},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375523},
doi = {10.1145/3375462.3375523},
abstract = {The use of computer-based systems in classrooms has provided teachers with new opportunities in delivering content to students, supplementing instruction, and assessing student knowledge and comprehension. Among the largest benefits of these systems is their ability to provide students with feedback on their work and also report student performance and progress to their teacher. While computer-based systems can automatically assess student answers to a range of question types, a limitation faced by many systems is in regard to open-ended problems. Many systems are either unable to provide support for open-ended problems, relying on the teacher to grade them manually, or avoid such question types entirely. Due to recent advancements in natural language processing methods, the automation of essay grading has made notable strides. However, much of this research has pertained to domains outside of mathematics, where the use of open-ended problems can be used by teachers to assess students' understanding of mathematical concepts beyond what is possible on other types of problems. This research explores the viability and challenges of developing automated graders of open-ended student responses in mathematics. We further explore how the scale of available data impacts model performance. Focusing on content delivered through the ASSISTments online learning platform, we present a set of analyses pertaining to the development and evaluation of models to predict teacher-assigned grades for student open responses.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics \& Knowledge},
pages = {615–624},
numpages = {10},
keywords = {automatic grading, open responses, natural language processing},
location = {Frankfurt, Germany},
series = {LAK '20}
}

 @article{schartel_2012, title={Giving feedback – An integral part of education}, volume={26}, DOI={10.1016/j.bpa.2012.02.003}, number={1}, journal={Best Practice \& Research Clinical Anaesthesiology}, author={Schartel, Scott A.}, year={2012}, pages={77–87}} 


@misc{malik2021generative,
      title={Generative Grading: Near Human-level Accuracy for Automated Feedback on Richly Structured Problems}, 
      author={Ali Malik and Mike Wu and Vrinda Vasavada and Jinpeng Song and Madison Coots and John Mitchell and Noah Goodman and Chris Piech},
      year={2021},
      eprint={1905.09916},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{rowe2008student,
  title={Student perceptions and preferences for feedback},
  author={Rowe, Anna D and Wood, Leigh N and others},
  year={2008},
  publisher={Canadian Centre of Science and Education}
}

@inproceedings{Holstein2018StudentLB,
  title={Student Learning Benefits of a Mixed-Reality Teacher Awareness Tool in AI-Enhanced Classrooms},
  author={Kenneth Holstein and B. McLaren and V. Aleven},
  booktitle={AIED},
  year={2018}
}

@article{Wang2015TowardsBA,
  title={Towards better affect detectors: effect of missing skills, class features and common wrong answers},
  author={Y. Wang and N. Heffernan and Cristina Heffernan},
  journal={Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
  year={2015}
}

@article{affect_detection,
author = {Ocumpaugh, Jaclyn and Baker, Ryan and Gowda, Sujith and Heffernan, Neil and Heffernan, Cristina},
title = {Population validity for educational data mining models: A case study in affect detection},
journal = {British Journal of Educational Technology},
volume = {45},
number = {3},
pages = {487-501},
doi = {https://doi.org/10.1111/bjet.12156},
url = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.12156},
eprint = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.12156},
abstract = {Abstract Information and communication technology (ICT)-enhanced research methods such as educational data mining (EDM) have allowed researchers to effectively model a broad range of constructs pertaining to the student, moving from traditional assessments of knowledge to assessment of engagement, meta-cognition, strategy and affect. The automated detection of these constructs allows EDM researchers to develop intervention strategies that can be implemented either by the software or the teacher. It also allows for secondary analyses of the construct, where the detectors are applied to a data set that is much larger than one that could be analyzed by more traditional methods. However, in many cases, the data used to develop EDM models are collected from students who may not be representative of the broader populations who are likely to use ICT. In order to use EDM models (automated detectors) with new populations, their generalizability must be verified. In this study, we examine whether detectors of affect remain valid when applied to new populations. Models of four educationally relevant affective states were constructed based on data from urban, suburban and rural students using ASSISTments software for middle school mathematics in the Northeastern United States. We found that affect detectors trained on a population drawn primarily from one demographic grouping do not generalize to populations drawn primarily from the other demographic groupings, even though those populations might be considered part of the same national or regional culture. Models constructed using data from all three subpopulations are more applicable to students in those populations than those trained on a single group, but still do not achieve ideal population validity—the ability to generalize across all subgroups. In particular, models generalize better across urban and suburban students than rural students. These findings have important implications for data collection efforts, validation techniques, and the design of interventions that are intended to be applied at scale.},
year = {2014}
}